# 🧠 Recurrent Neural Network from Scratch

This repository contains a simple yet powerful implementation of a Recurrent Neural Network (RNN) built from scratch using **NumPy**, without any deep learning libraries like TensorFlow or PyTorch. The main goal of this project is to demystify how RNNs work internally by manually coding forward and backward propagation steps.

## 📚 What You'll Learn

- How sequential data is processed using RNNs
- Implementation of forward propagation through time
- Manual backpropagation through time (BPTT)
- Use of tanh activation and softmax for output prediction
- How weights and gradients are computed and updated manually
- Visual breakdown of gradients across time steps

## 🔁 Core Concepts Demonstrated

- **Forward pass:** Calculation of hidden states and outputs across sequence inputs
- **Loss computation:** Using cross-entropy loss for classification
- **Backpropagation:** Step-by-step gradient calculations using chain rule and tanh derivatives
- **Weight updates:** Applying gradient descent manually

## 🔧 Technologies Used

- Python 🐍
- NumPy 🔢
- Jupyter Notebook 📓


## 💡 Motivation
This project was built to solidify understanding of how RNNs operate internally and serve as a learning resource for others trying to dive deeper into neural networks without black-box abstractions.

