# ğŸ§  Recurrent Neural Network from Scratch

This repository contains a simple yet powerful implementation of a Recurrent Neural Network (RNN) built from scratch using **NumPy**, without any deep learning libraries like TensorFlow or PyTorch. The main goal of this project is to demystify how RNNs work internally by manually coding forward and backward propagation steps.

## ğŸ“š What You'll Learn

- How sequential data is processed using RNNs
- Implementation of forward propagation through time
- Manual backpropagation through time (BPTT)
- Use of tanh activation and softmax for output prediction
- How weights and gradients are computed and updated manually
- Visual breakdown of gradients across time steps

## ğŸ” Core Concepts Demonstrated

- **Forward pass:** Calculation of hidden states and outputs across sequence inputs
- **Loss computation:** Using cross-entropy loss for classification
- **Backpropagation:** Step-by-step gradient calculations using chain rule and tanh derivatives
- **Weight updates:** Applying gradient descent manually

## ğŸ”§ Technologies Used

- Python ğŸ
- NumPy ğŸ”¢
- Jupyter Notebook ğŸ““


## ğŸ’¡ Motivation
This project was built to solidify understanding of how RNNs operate internally and serve as a learning resource for others trying to dive deeper into neural networks without black-box abstractions.

