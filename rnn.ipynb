{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb92bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e12abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>rain</th>\n",
       "      <th>tmax_tomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01</th>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02</th>\n",
       "      <td>52.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-03</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-04</th>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-05</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tmax  tmin  rain  tmax_tomorrow\n",
       "1970-01-01  60.0  35.0   0.0           52.0\n",
       "1970-01-02  52.0  39.0   0.0           52.0\n",
       "1970-01-03  52.0  35.0   0.0           53.0\n",
       "1970-01-04  53.0  36.0   0.0           52.0\n",
       "1970-01-05  52.0  35.0   0.0           50.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"clean_weather.csv\",index_col=0)\n",
    "data = data.ffill()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a9d74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66., 70., 62.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps = data[\"tmax\"].tail(3).to_numpy()\n",
    "temps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74818a",
   "metadata": {},
   "source": [
    "\n",
    "### Forward Pass and Prediction\n",
    "\n",
    "Let's go through an example to see how this works. We'll initialize each weight matrix, then perform a sample forward pass with 3 sequence elements:\n",
    "\n",
    "1. **Input Weights (`i_weights`)**: These weights connect the input to the hidden layer.\n",
    "2. **Hidden Weights (`h_weights`)**: These weights connect the hidden state from the previous time step to the current hidden state.\n",
    "3. **Output Weights (`o_weights`)**: These weights connect the hidden layer to the output.\n",
    "\n",
    "The forward pass involves calculating the following for each time step:\n",
    "- **Input to Hidden (`XI_t`)**: The contribution of the input at the current time step to the hidden state.\n",
    "- **Hidden State (`XH_t`)**: The combined effect of the input and the previous hidden state, passed through an activation function (ReLU in this case).\n",
    "- **Output (`XO_t`)**: The prediction for the next sequence element, based on the current hidden state.\n",
    "\n",
    "We perform this process for 3 sequence elements (`x0`, `x1`, `x2`) using the initialized weights and inputs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65063b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "x0 = temps[0].reshape(1,1)\n",
    "x1 = temps[1].reshape(1,1)\n",
    "x2 = temps[2].reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0f0cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43758721],\n",
       "       [0.891773  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights\n",
    "np.random.seed(0)\n",
    "i_weights = np.random.rand(1,2)\n",
    "h_weights = np.random.rand(2,2)\n",
    "o_weights = np.random.rand(2,1)\n",
    "o_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377f71d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.22169126, 47.20249818]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate xi at time step 0\n",
    "XI_0 = x0 @ i_weights\n",
    "# there is no previous time step, so there is not going to be a hidden state\n",
    "XH_0 = np.maximum(0, XI_0) # relu\n",
    "# Output at time step 0 at xo_0\n",
    "XO_0 = XH_0 @ o_weights\n",
    "XH_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf2e95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124.54916092]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XI_1 = x1 @ i_weights\n",
    "XH = XH_0 @ h_weights\n",
    "XH_1 = np.maximum(0,XH+XI_1)\n",
    "XO_1 = XH_1@ o_weights\n",
    "XO_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0cad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[190.94853131]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XI_2 = x2 @ i_weights\n",
    "XH = XH_1 @ h_weights\n",
    "XH_2 = np.maximum(0,XH+XI_2)\n",
    "XO_2 = XH_2@ o_weights\n",
    "XO_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d579dec",
   "metadata": {},
   "source": [
    "We've now passed through 3 forward steps of our RNN! The output x0 at each time step is the prediction for the next element in the sequence.\n",
    "\n",
    "The hidden state of the RNN allows the network to have information about all past sequence elements. So when we're processing the sequence item at time step 2, the hidden state of the RNN stores information about the sequence elements at time step 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95094742",
   "metadata": {},
   "source": [
    "## Forward pass but with tanh as an activation function also lets make a function to do the forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbbce2",
   "metadata": {},
   "source": [
    "We'll also scale the weights and the bias to work properly with tanh nonlinearity. and we'll make our input and hidden weights small, so tanh doesn't squash all the values to 1 or -1. we'll also make the output weight large, since the output of the hidden step will be small. and yeah RNN, the network would eventually learn the correct parameters. but initializing the weights this way is a good start and helps with the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd4b54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# weights and bias\n",
    "#scaling the weights and bias so values get through tanh nonlinearity\n",
    "i_weights = np.random.rand(1,5) / 5 - .1 # small weights\n",
    "h_weights = np.random.rand(5,5)/ 5 - .1 # small weights\n",
    "o_weights = np.random.rand(5,1) * 50 #large weights\n",
    "h_bias = np.random.rand(1,5)/ 5 - .1\n",
    "o_bias = np.random.rand(1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ecdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_23208\\2267004475.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  outputs[i,] = XO\n"
     ]
    }
   ],
   "source": [
    "#lets run a the forward pass on a loop\n",
    "# also, this loop will process sequence elements one by and store the output predictoin and the hidden state\n",
    "\n",
    "# array to store outputs and hiddens states\n",
    "outputs = np.zeros(3)\n",
    "hiddens = np.zeros((3,5))\n",
    "sequence = data[\"tmax\"].tail(3).to_numpy()\n",
    "prev_hidden = None\n",
    "\n",
    "for i in range(3):\n",
    "    x  =  sequence[i].reshape(1,1)\n",
    "\n",
    "    XI = x @ i_weights\n",
    "    if prev_hidden is None:\n",
    "        XH = XI\n",
    "    else:\n",
    "        XH = XH + prev_hidden @ h_weights + h_bias\n",
    "    \n",
    "    # activation fn\n",
    "    XH = np.tanh(XH)\n",
    "    prev_hidden = XH\n",
    "    hiddens[i,] = XH\n",
    "\n",
    "    XO = XH @ o_weights + o_bias\n",
    "    outputs[i,] = XO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2062617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.68122178, 72.99311119, 69.28433643])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1741ed29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56784618,  0.99320288,  0.87557333,  0.53166114, -0.76483255],\n",
       "       [ 0.55326914,  0.76848706,  0.73664379,  0.68197976, -0.65821463],\n",
       "       [ 0.55377633,  0.66710561,  0.66350698,  0.74597413, -0.59604942]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8198617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual,predicted):\n",
    "    return np.mean((actual - predicted)**2)\n",
    "\n",
    "def grad_mse(actual,predicted):\n",
    "    return (actual-predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a365ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.68122178, -10.99311119,  -4.28433643])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = np.array([70, 62, 65])\n",
    "loss_grad = grad_mse(actuals,outputs)\n",
    "loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256738a",
   "metadata": {},
   "source": [
    "\n",
    "### Backpropagation in RNNs\n",
    "\n",
    "Backpropagation in RNNs involves calculating the gradients of the loss function with respect to the weights (`i_weights`, `h_weights`, `o_weights`) and biases (`h_bias`, `o_bias`) through time. This process, called Backpropagation Through Time (BPTT), adjusts the parameters to minimize the prediction error (`outputs` vs. actual values). Gradients are computed by propagating errors backward from the output layer to the hidden states (`hiddens`) and inputs (`sequence`), considering the temporal dependencies in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbc5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629875a8",
   "metadata": {},
   "source": [
    "## manual backpropagation for a single timestep (t=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47216121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loss wrt the output at the current time step\n",
    "l2_grad = loss_grad[2].reshape(1,1)\n",
    "\n",
    "# Add to the output weight gradient\n",
    "# Multiply the output of the hidden step (hiddens[2]) transposed by the l2 grad\n",
    "# np.newaxis creates a new size 1 axis, effectively transposing the hiddens\n",
    "o_weight_grad += hiddens[2][:,np.newaxis] @ l2_grad\n",
    "# Add to the bias gradient.  Similar to a dense neural network, this is just the mean of the l2_grad.\n",
    "o_bias_grad += np.mean(l2_grad)\n",
    "\n",
    "h2_grad = l2_grad @ o_weights.T\n",
    "\n",
    "#derivative of tanh\n",
    "tanh_deriv = 1 - hiddens[2,:][np.newaxis,:] **2\n",
    "# Multiply each position in the h_grad by the tanh derivative - this \"undoes\" the tanh in the forward pass\n",
    "h2_grad = np.multiply(h2_grad,tanh_deriv)\n",
    "\n",
    "\n",
    "# Now, find how much we need to update the hidden weights.\n",
    "# We take the input to the hidden step (the output of the previous hidden step in the forward pass) @ h2_grad\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h2_grad\n",
    "h_bias_grad += np.mean(h2_grad)\n",
    "# This multiples the sequence value at time step 2 by the gradient\n",
    "# We don't need the .T here, but I left it here in case you have a larger input size\n",
    "i_weight_grad += sequence[2].reshape(1,1).T @ h2_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335bb2e",
   "metadata": {},
   "source": [
    "## manual backpropagation for a single timestep (t=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49205fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ea2d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_grad = loss_grad[1].reshape(1,1)\n",
    "o_weight_grad += hiddens[1][:,np.newaxis] @l1_grad\n",
    "o_bias_grad += np.mean(l1_grad)\n",
    "\n",
    "\n",
    "h1_grad = l1_grad @ o_weights.T\n",
    "\n",
    "h1_grad += h2_grad @ h_weights.T \n",
    "\n",
    "tanh_deriv = 1 - hiddens[1,:][np.newaxis,:] **2\n",
    "h1_grad = np.multiply(h1_grad,tanh_deriv)\n",
    "\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h1_grad\n",
    "h_bias_grad += np.mean(h1_grad)\n",
    "i_weight_grad += sequence[1].reshape(1,1).T @ h1_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0ee33",
   "metadata": {},
   "source": [
    "Now, we can do the final sequence position, 0. The main difference here is that we don't update the hidden gradient, since there is no previous sequence position that gave us hidden state input in the forward pass:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b2662",
   "metadata": {},
   "source": [
    "## manual backpropagation for a single timestep (t=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a70746c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_grad = loss_grad[0].reshape(1,1)\n",
    "o_weight_grad += hiddens[0][:,np.newaxis] @l0_grad\n",
    "o_bias_grad += np.mean(l0_grad)\n",
    "\n",
    "h0_grad = l0_grad @ o_weights.T\n",
    "h0_grad = h1_grad @ h_weights.T\n",
    "\n",
    "tanh_deriv = 1 - hiddens[0,:][np.newaxis,:] ** 2\n",
    "h0_grad = np.multiply(h0_grad,tanh_deriv)\n",
    "i_weight_grad += sequence[0].reshape(1,1).T @ h0_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06509fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11095.10131226, -18151.59749784, -11121.5552512 ,\n",
       "        -16872.95024558,   -581.47427619]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e15fde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hidden = None\n",
    "\n",
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5\n",
    "\n",
    "for i in range(2, -1, -1):\n",
    "    l_grad = loss_grad[i].reshape(1,1)\n",
    "\n",
    "    o_weight_grad += hiddens[i][:,np.newaxis] @ l_grad\n",
    "    o_bias_grad += np.mean(l_grad)\n",
    "\n",
    "    o_grad = l_grad @ o_weights.T\n",
    "\n",
    "    # Only add in the hidden gradient if a next sequence exists\n",
    "    if next_hidden is not None:\n",
    "        h_grad = o_grad + next_hidden @ h_weights.T\n",
    "    else:\n",
    "        h_grad = o_grad\n",
    "\n",
    "    tanh_deriv = 1 - hiddens[i,:][np.newaxis,:] ** 2\n",
    "    h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "    next_hidden = h_grad\n",
    "\n",
    "    # Don't update the hidden weights for the first sequence position\n",
    "    if i > 0:\n",
    "        h_weight_grad += hiddens[i-1,:][:,np.newaxis] @ h_grad\n",
    "        h_bias_grad += np.mean(h_grad)\n",
    "\n",
    "    i_weight_grad += sequence[i].reshape(1,1).T @ h_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "839996cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-6\n",
    "# We'll divide the learning rate by the sequence length, since we were adding together the gradients\n",
    "# This makes training the model more stable\n",
    "lr = lr / 3\n",
    "\n",
    "i_weights -= i_weight_grad * lr\n",
    "h_weights -= h_weight_grad * lr\n",
    "h_bias -= h_bias_grad * lr\n",
    "o_weights -= o_weight_grad * lr\n",
    "o_bias -= o_bias_grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3076fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.22778545],\n",
       "       [38.71169178],\n",
       "       [22.80752338],\n",
       "       [28.4217029 ],\n",
       "       [ 0.93948404]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2ee2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_23208\\2267004475.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  outputs[i,] = XO\n"
     ]
    }
   ],
   "source": [
    "#lets run a the forward pass on a loop\n",
    "# also, this loop will process sequence elements one by and store the output predictoin and the hidden state\n",
    "\n",
    "# array to store outputs and hiddens states\n",
    "outputs = np.zeros(3)\n",
    "hiddens = np.zeros((3,5))\n",
    "sequence = data[\"tmax\"].tail(3).to_numpy()\n",
    "prev_hidden = None\n",
    "\n",
    "for i in range(3):\n",
    "    x  =  sequence[i].reshape(1,1)\n",
    "\n",
    "    XI = x @ i_weights\n",
    "    if prev_hidden is None:\n",
    "        XH = XI\n",
    "    else:\n",
    "        XH = XH + prev_hidden @ h_weights + h_bias\n",
    "    \n",
    "    # activation fn\n",
    "    XH = np.tanh(XH)\n",
    "    prev_hidden = XH\n",
    "    hiddens[i,] = XH\n",
    "\n",
    "    XO = XH @ o_weights + o_bias\n",
    "    outputs[i,] = XO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e1d1583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([94.0957397 , 80.16647914, 73.3373401 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "964119ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "\n",
    "# Define predictors and target\n",
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "# Scale our data to have mean 0\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "# Split into train, valid, test sets\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0e1dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        np.random.seed(0)\n",
    "        k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8146057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    hiddens = []\n",
    "    outputs = []\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
    "        for j in range(x.shape[0]):\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
    "            # Activation.  tanh avoids outputs getting larger and larger.\n",
    "            hidden_x = np.tanh(hidden_x)\n",
    "            # Store hidden for use in backprop\n",
    "            hidden[j,:] = hidden_x\n",
    "\n",
    "            # Output layer\n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            output[j,:] = output_x\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "    return hiddens, outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb975f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = hiddens[i]\n",
    "        next_h_grad = None\n",
    "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):\n",
    "            # Add newaxis in the first dimension\n",
    "            out_grad = grad[j,:][np.newaxis, :]\n",
    "\n",
    "            # Output updates\n",
    "            # np.newaxis creates a size 1 axis, in this case transposing matrix\n",
    "            o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate gradient to hidden unit\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                # Add the gradients together to combine output contribution and hidden contribution\n",
    "                h_grad += hh_grad\n",
    "\n",
    "            # Pull the gradient across the current hidden nonlinearity\n",
    "            # derivative of tanh is 1 - tanh(x) ** 2\n",
    "            # So we take the output of tanh (next hidden state), and plug in\n",
    "            tanh_deriv = 1 - hidden[j][np.newaxis,:] ** 2\n",
    "\n",
    "            # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
    "            # Effect is to pull value across nonlinearity\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            # Store to compute h grad for previous sequence position\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            # If we're not at the very beginning\n",
    "            if j > 0:\n",
    "                # Multiply input from previous layer by post-nonlinearity grad at current layer\n",
    "                h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
    "\n",
    "        # Normalize lr by number of sequence elements\n",
    "        lr = lr / x.shape[0]\n",
    "        i_weight -= i_weight_grad * lr\n",
    "        h_weight -= h_weight_grad * lr\n",
    "        h_bias -= h_bias_grad * lr\n",
    "        o_weight -= o_weight_grad * lr\n",
    "        o_bias -= o_bias_grad * lr\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82ddd298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss 5474.247764988007 valid loss 8072.211100544781\n",
      "Epoch: 50 train loss 2.5575602781382102e+20 valid loss 2.8110955860398418e+20\n",
      "Epoch: 100 train loss 1.1684645414633017e+31 valid loss 1.3518368388070007e+31\n",
      "Epoch: 150 train loss 4.863333964498297e+41 valid loss 5.628058894187397e+41\n",
      "Epoch: 200 train loss 2.0241764958706716e+52 valid loss 2.3424652126513832e+52\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "lr = 1e-5\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": 3},\n",
    "    {\"type\": \"rnn\", \"hidden\": 4, \"output\": 1}\n",
    "]\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sequence_len = 7\n",
    "    epoch_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j:(j+sequence_len),]\n",
    "        seq_y = train_y[j:(j+sequence_len),]\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = grad_mse(seq_y, outputs)\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        sequence_len = 7\n",
    "        valid_loss = 0\n",
    "        for j in range(valid_x.shape[0] - sequence_len):\n",
    "            seq_x = valid_x[j:(j+sequence_len),]\n",
    "            seq_y = valid_y[j:(j+sequence_len),]\n",
    "            _, outputs = forward(seq_x, layers)\n",
    "            valid_loss += mse(seq_y, outputs)\n",
    "\n",
    "        print(f\"Epoch: {epoch} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09d271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
