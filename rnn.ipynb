{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb92bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e12abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>rain</th>\n",
       "      <th>tmax_tomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01</th>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02</th>\n",
       "      <td>52.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-03</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-04</th>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-05</th>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tmax  tmin  rain  tmax_tomorrow\n",
       "1970-01-01  60.0  35.0   0.0           52.0\n",
       "1970-01-02  52.0  39.0   0.0           52.0\n",
       "1970-01-03  52.0  35.0   0.0           53.0\n",
       "1970-01-04  53.0  36.0   0.0           52.0\n",
       "1970-01-05  52.0  35.0   0.0           50.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"clean_weather.csv\",index_col=0)\n",
    "data = data.ffill()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29a9d74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66., 70., 62.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps = data[\"tmax\"].tail(3).to_numpy()\n",
    "temps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74818a",
   "metadata": {},
   "source": [
    "\n",
    "### Forward Pass and Prediction\n",
    "\n",
    "Let's go through an example to see how this works. We'll initialize each weight matrix, then perform a sample forward pass with 3 sequence elements:\n",
    "\n",
    "1. **Input Weights (`i_weights`)**: These weights connect the input to the hidden layer.\n",
    "2. **Hidden Weights (`h_weights`)**: These weights connect the hidden state from the previous time step to the current hidden state.\n",
    "3. **Output Weights (`o_weights`)**: These weights connect the hidden layer to the output.\n",
    "\n",
    "The forward pass involves calculating the following for each time step:\n",
    "- **Input to Hidden (`XI_t`)**: The contribution of the input at the current time step to the hidden state.\n",
    "- **Hidden State (`XH_t`)**: The combined effect of the input and the previous hidden state, passed through an activation function (ReLU in this case).\n",
    "- **Output (`XO_t`)**: The prediction for the next sequence element, based on the current hidden state.\n",
    "\n",
    "We perform this process for 3 sequence elements (`x0`, `x1`, `x2`) using the initialized weights and inputs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65063b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "x0 = temps[0].reshape(1,1)\n",
    "x1 = temps[1].reshape(1,1)\n",
    "x2 = temps[2].reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c0f0cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43758721],\n",
       "       [0.891773  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights\n",
    "np.random.seed(0)\n",
    "i_weights = np.random.rand(1,2)\n",
    "h_weights = np.random.rand(2,2)\n",
    "o_weights = np.random.rand(2,1)\n",
    "o_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "377f71d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.22169126, 47.20249818]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate xi at time step 0\n",
    "XI_0 = x0 @ i_weights\n",
    "# there is no previous time step, so there is not going to be a hidden state\n",
    "XH_0 = np.maximum(0, XI_0) # relu\n",
    "# Output at time step 0 at xo_0\n",
    "XO_0 = XH_0 @ o_weights\n",
    "XH_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cf2e95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124.54916092]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XI_1 = x1 @ i_weights\n",
    "XH = XH_0 @ h_weights\n",
    "XH_1 = np.maximum(0,XH+XI_1)\n",
    "XO_1 = XH_1@ o_weights\n",
    "XO_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d0cad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[190.94853131]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XI_2 = x2 @ i_weights\n",
    "XH = XH_1 @ h_weights\n",
    "XH_2 = np.maximum(0,XH+XI_2)\n",
    "XO_2 = XH_2@ o_weights\n",
    "XO_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d579dec",
   "metadata": {},
   "source": [
    "We've now passed through 3 forward steps of our RNN! The output x0 at each time step is the prediction for the next element in the sequence.\n",
    "\n",
    "The hidden state of the RNN allows the network to have information about all past sequence elements. So when we're processing the sequence item at time step 2, the hidden state of the RNN stores information about the sequence elements at time step 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95094742",
   "metadata": {},
   "source": [
    "## Forward pass but with tanh as an activation function also lets make a function to do the forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbbce2",
   "metadata": {},
   "source": [
    "We'll also scale the weights and the bias to work properly with tanh nonlinearity. and we'll make our input and hidden weights small, so tanh doesn't squash all the values to 1 or -1. we'll also make the output weight large, since the output of the hidden step will be small. and yeah RNN, the network would eventually learn the correct parameters. but initializing the weights this way is a good start and helps with the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd4b54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# weights and bias\n",
    "#scaling the weights and bias so values get through tanh nonlinearity\n",
    "i_weights = np.random.rand(1,5) / 5 - .1 # small weights\n",
    "h_weights = np.random.rand(5,5)/ 5 - .1 # small weights\n",
    "o_weights = np.random.rand(5,1) * 50 #large weights\n",
    "h_bias = np.random.rand(1,5)/ 5 - .1\n",
    "o_bias = np.random.rand(1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25ecdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_17480\\2267004475.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  outputs[i,] = XO\n"
     ]
    }
   ],
   "source": [
    "#lets run a the forward pass on a loop\n",
    "# also, this loop will process sequence elements one by and store the output predictoin and the hidden state\n",
    "\n",
    "# array to store outputs and hiddens states\n",
    "outputs = np.zeros(3)\n",
    "hiddens = np.zeros((3,5))\n",
    "sequence = data[\"tmax\"].tail(3).to_numpy()\n",
    "prev_hidden = None\n",
    "\n",
    "for i in range(3):\n",
    "    x  =  sequence[i].reshape(1,1)\n",
    "\n",
    "    XI = x @ i_weights\n",
    "    if prev_hidden is None:\n",
    "        XH = XI\n",
    "    else:\n",
    "        XH = XH + prev_hidden @ h_weights + h_bias\n",
    "    \n",
    "    # activation fn\n",
    "    XH = np.tanh(XH)\n",
    "    prev_hidden = XH\n",
    "    hiddens[i,] = XH\n",
    "\n",
    "    XO = XH @ o_weights + o_bias\n",
    "    outputs[i,] = XO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2062617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.68122178, 72.99311119, 69.28433643])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1741ed29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56784618,  0.99320288,  0.87557333,  0.53166114, -0.76483255],\n",
       "       [ 0.55326914,  0.76848706,  0.73664379,  0.68197976, -0.65821463],\n",
       "       [ 0.55377633,  0.66710561,  0.66350698,  0.74597413, -0.59604942]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8198617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual,predicted):\n",
    "    return np.mean((actual - predicted)**2)\n",
    "\n",
    "def grad_mse(actual,predicted):\n",
    "    return (actual-predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a365ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.68122178, -10.99311119,  -4.28433643])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = np.array([70, 62, 65])\n",
    "loss_grad = grad_mse(actuals,outputs)\n",
    "loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256738a",
   "metadata": {},
   "source": [
    "\n",
    "### Backpropagation in RNNs\n",
    "\n",
    "Backpropagation in RNNs involves calculating the gradients of the loss function with respect to the weights (`i_weights`, `h_weights`, `o_weights`) and biases (`h_bias`, `o_bias`) through time. This process, called Backpropagation Through Time (BPTT), adjusts the parameters to minimize the prediction error (`outputs` vs. actual values). Gradients are computed by propagating errors backward from the output layer to the hidden states (`hiddens`) and inputs (`sequence`), considering the temporal dependencies in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2dbc5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629875a8",
   "metadata": {},
   "source": [
    "## manual backpropagation for a single timestep (t=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47216121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loss wrt the output at the current time step\n",
    "l2_grad = loss_grad[2].reshape(1,1)\n",
    "\n",
    "# Add to the output weight gradient\n",
    "# Multiply the output of the hidden step (hiddens[2]) transposed by the l2 grad\n",
    "# np.newaxis creates a new size 1 axis, effectively transposing the hiddens\n",
    "o_weight_grad += hiddens[2][:,np.newaxis] @ l2_grad\n",
    "# Add to the bias gradient.  Similar to a dense neural network, this is just the mean of the l2_grad.\n",
    "o_bias_grad += np.mean(l2_grad)\n",
    "\n",
    "h2_grad = l2_grad @ o_weights.T\n",
    "\n",
    "#derivative of tanh\n",
    "tanh_deriv = 1 - hiddens[2,:][np.newaxis,:] **2\n",
    "# Multiply each position in the h_grad by the tanh derivative - this \"undoes\" the tanh in the forward pass\n",
    "h2_grad = np.multiply(h2_grad,tanh_deriv)\n",
    "\n",
    "\n",
    "# Now, find how much we need to update the hidden weights.\n",
    "# We take the input to the hidden step (the output of the previous hidden step in the forward pass) @ h2_grad\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h2_grad\n",
    "h_bias_grad += np.mean(h2_grad)\n",
    "# This multiples the sequence value at time step 2 by the gradient\n",
    "# We don't need the .T here, but I left it here in case you have a larger input size\n",
    "i_weight_grad += sequence[2].reshape(1,1).T @ h2_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335bb2e",
   "metadata": {},
   "source": [
    "## manual backpropagation for a single timestep (t=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49205fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ea2d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_grad = loss_grad[1].reshape(1,1)\n",
    "o_weight_grad += hiddens[1][:,np.newaxis] @l1_grad\n",
    "o_bias_grad += np.mean(l1_grad)\n",
    "\n",
    "\n",
    "h1_grad = l1_grad @ o_weights.T\n",
    "\n",
    "h1_grad += h2_grad @ h_weights.T \n",
    "\n",
    "tanh_deriv = 1 - hiddens[1,:][np.newaxis,:] **2\n",
    "h1_grad = np.multiply(h1_grad,tanh_deriv)\n",
    "\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h1_grad\n",
    "h_bias_grad += np.mean(h1_grad)\n",
    "i_weight_grad += sequence[1].reshape(1,1).T @ h1_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0ee33",
   "metadata": {},
   "source": [
    "Now, we can do the final sequence position, 0. The main difference here is that we don't update the hidden gradient, since there is no previous sequence position that gave us hidden state input in the forward pass:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b2662",
   "metadata": {},
   "source": [
    "## manual backpropagation for a single timestep (t=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a70746c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_grad = loss_grad[0].reshape(1,1)\n",
    "o_weight_grad += hiddens[0][:,np.newaxis] @l0_grad\n",
    "o_bias_grad += np.mean(l0_grad)\n",
    "\n",
    "h0_grad = l0_grad @ o_weights.T\n",
    "h0_grad = h1_grad @ h_weights.T\n",
    "\n",
    "tanh_deriv = 1 - hiddens[0,:][np.newaxis,:] ** 2\n",
    "h0_grad = np.multiply(h0_grad,tanh_deriv)\n",
    "i_weight_grad += sequence[0].reshape(1,1).T @ h0_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06509fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11095.10131226, -18151.59749784, -11121.5552512 ,\n",
       "        -16872.95024558,   -581.47427619]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fde6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
